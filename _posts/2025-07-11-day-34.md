---
layout: post
title: "Day 34 – Interpol"
date: 2025-07-11
author: Elton Mawire
permalink: /day34.html
tags: ["LSTM", "CNN", "ANN", "RNN"]

what_i_learned: |
  Today I trained Random Forest and Gradient Boosting models using my interpolated PM2.5 dataset, which includes features like AOD, temperature, humidity, and wind speed. Both models produced unexpectedly high performance scores, which made me question the validity of the results. I learned that when working with interpolated or smoothed data, it’s important to consider how that data might make it easier for models to find patterns that don't generalize to real-world situations. This led me to start thinking more critically about how data quality and preprocessing can influence model outcomes.

blockers: |
  No blockers

reflection: |
  Although the models showed strong predictive power, I now realize that high accuracy doesn’t always mean a model is reliable. When using interpolated datasets. Interpolation can introduce artificial smoothness or redundancy, which might lead to overfitting or learning patterns that wouldn’t exist in real, noisy observations. This experience taught me the importance of being skeptical of perfect-looking results and validating models on independent or non-interpolated data to truly assess performance.
---
