---
layout: post
title: "Day 33 – Neural Networks"
date: 2025-07-10
author: Elton Mawire
permalink: /day33.html
tags: ["LSTM", "CNN", "ANN", "RNN"]

what_i_learned: |
  Today, I focused on preparing my PM2.5 dataset for use with LSTM models. I learned how to structure time series data by turning it into overlapping sequences so that the model can understand patterns over time. Specifically, the use of sliding windows of past values (like AOD, temperature, humidity, and wind speed) to predict future PM2.5 levels.

  I also practiced normalizing the features and target values using MinMaxScaler so that the data fits within a range that's friendly to neural networks. This step is especially important for LSTMs, which are sensitive to the scale of input values.

  One key insight was understanding the structure expected by LSTM models: they need 3D input of the form (batch_size, time_steps, features). I created a function to generate these sequences by sliding a window across the dataset and pairing each window with the PM2.5 value that follows it.

blockers: |
  No Blockers

reflection: |
  This part of the project really helped me understand how raw data needs to be shaped before it can be used in machine learning models — especially for sequential models like LSTMs. At first, it wasn’t intuitive that we need to manually slide windows over the data and realign the target values. But once I implemented the create_sequences() function and saw how it worked, it all clicked.Another interesting takeaway was seeing how both PyTorch and TensorFlow handle the same concept but in slightly different ways — PyTorch gives more control, while TensorFlow abstracts a lot under the hood. Both are powerful depending on the use case.
---
