---
layout: post
title: "Day 33 – Neural Networks"
date: 2025-07-10
author: Elton Mawire
permalink: /day33.html
tags: ["LSTM", "CNN", "ANN", "RNN"]

what_i_learned: |
  Today, I focused on preparing my PM2.5 dataset for use with LSTM models. I learned how to structure time series data by turning it into overlapping sequences so that the model can understand patterns over time. Specifically, the use of sliding windows of past values (like AOD, temperature, humidity, and wind speed) to predict future PM2.5 levels.

  I also practiced normalizing the features and target values using MinMaxScaler so that the data fits within a range that's friendly to neural networks. This step is especially important for LSTMs, which are sensitive to the scale of input values.

  One key insight was understanding the structure expected by LSTM models: they need 3D input of the form (batch_size, time_steps, features). I created a function to generate these sequences by sliding a window across the dataset and pairing each window with the PM2.5 value that follows it.

blockers: |
  No Blockers

reflection: |
  Although I haven't implemented the models yet, learning how to prepare data for LSTM networks gave me a much clearer understanding of how time series forecasting works. I now understand that LSTMs require the input data to be structured as sequences, and that we need to create those sequences manually using a sliding window approach. It was also eye-opening to realize that the model learns to predict the next PM2.5 value based on several previous readings, which makes the data structure critical.I also got a good grasp of how data scaling works and why it's necessary before feeding anything into a neural network. Even though I haven’t trained the model yet, I feel much more confident about what preprocessing steps are needed and how frameworks like PyTorch and TensorFlow handle this kind of input.
---
